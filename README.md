# Research__GD-OCC
GD-OCC: Geometry-driven 3D Occupancy Prediction With Integrated View Transformation and Sparse Representation
# OPUS
### 摘要：
<font style="color:rgb(0, 0, 0);">占用预测旨在预测在体素化3D环境中的占用状态正在迅速在自主驾驶社区内获得动力。主流占用预测首先将3D环境离散为体素，然后对这种</font>**<font style="color:rgb(0, 0, 0);">密集</font>**<font style="color:rgb(0, 0, 0);">的网格进行分类。</font><font style="color:#DF2A3F;">（目前的现状）</font><font style="color:rgb(0, 0, 0);">但是，对样本数据的检查表明，绝大多数体素没有占用。（</font><font style="color:#DF2A3F;">有什么问题）</font><font style="color:rgb(0, 0, 0);">对这些空素的进行分类需要次优的计算资源分配，并减少这样的空素素需要复杂的算法设计。</font><font style="color:#DF2A3F;">（衔接，提供解决这一问题的引导）</font><font style="color:rgb(0, 0, 0);">为此，我们介绍了有关占用预测任务的新观点：将其作为简化的设置预测范式制定，而无需明确的空间建模或复杂的稀疏程序。我们提出的称为Opus的框架利用Transformer编码器编码器体系结构同时使用一组可学习的查询来预测被占领的位置和类。首先，我们采用倒角距离损失来扩展定居的比较问题到前所未有的幅度，从而使这种模型端到端的训练成为现实。随后，使用基于学到的位置的最近的邻居搜索自适应地分配语义类。此外，Opus还结合了一系列非平凡策略，以增强模型性能，包括粗到精细的学习，一致的点采样和自适应重新加权。</font><font style="color:#DF2A3F;">（介绍方法，更像是流程？）</font><font style="color:rgb(0, 0, 0);">最后，与当前最先进的方法相比，我们最亮的模型在接近2×fps的OCC3D-Nuscenes数据集上获得了优越的Rayiou，而我们最重的模型超过了先前的最佳结果6.1 Rayiou。</font><font style="color:#DF2A3F;">（实验结果）</font>

### 引言：逻辑
第一段：相比三维目标检测，占用预测能更好的提供场景的几何形状与语义信息。+ 举例

第二段：基于密集的方法，没有必要，举例<font style="color:rgb(0, 0, 0);">Semantickitti 和Occ3d-nuscenes两个数据集。目前尝试用tpv表示尝试解决这个问题，但依旧存在问题。</font>

<font style="color:rgb(0, 0, 0);">第三段：将任务制定为直接设置的预测问题，给出流程（1）骨干网络提取特征（2）一组查询，预测出位置与语义（3）稀疏解码器更新查询。认为在计算loss时，匈牙利匹配不合适无法处理大量体素。</font>

<font style="color:rgb(0, 0, 0);">第四段：一对一关联没有必要，给出创新点，降低复杂度。</font>

<font style="color:rgb(0, 0, 0);">第五段：此外，一些提升性能的方法介绍。介绍实验精度。</font>

<font style="color:rgb(0, 0, 0);">创新点1,2,3……</font>

### <font style="color:rgb(0, 0, 0);">相关工作</font>
占用预测

用transfomer进行集合预测

# Occupancy as Set of Points
### 摘要
<font style="color:rgb(0, 0, 0);">在本文中，我们探讨了从多视图图像中的3D占用预测的新颖点表示，该预测被称为占用率。</font><font style="color:#DF2A3F;">（任务介绍）</font><font style="color:rgb(0, 0, 0);">现有的基于摄像头的方法倾向于利用基于体素的密集表示以预测整个场景的占用，</font><font style="color:#DF2A3F;">因此很难专注于感知范围内的特殊区域或区域</font><font style="color:rgb(0, 0, 0);">。相比之下，我们提出了代表场景并提出OSP的兴趣点（POI），这是基于点3D占用预测的新型框架。由于基于点表示的固有灵活性，与现有方法相比，OSP在训练和推理适应性方面表现出色。它扩展到了传统的感知边界，并且可以与基于音量的方法无缝集成以显着提高其有效性。 OCC3D-Nuscenes占用基准的实验表明，OSP具有强大的性能和灵活性。</font>

### <font style="color:rgb(0, 0, 0);">引言</font>
<font style="color:rgb(0, 0, 0);">第一段</font><font style="color:#DF2A3F;">（点明任务）</font><font style="color:rgb(0, 0, 0);">：整体3D场景的理解对于自主驾驶系统至关重要，直接影响后续任务的效率和准确性。考虑到与其他传感器相比，考虑到相机的成本效益和易于部署的易用性，开发基于视觉的3D场景理解方法已成为一个重大且广泛研究的挑战。</font>

<font style="color:rgb(0, 0, 0);">第二段</font><font style="color:#DF2A3F;">（还是任务描述）</font><font style="color:rgb(0, 0, 0);">：为了应对这一挑战，已经提出并广泛研究了3D语义场景（SSC）[22] [22]，以共同从有限的观测值中推断现场的几何形状和语义信息。 SSC任务要求模型准确预测可见位置并完成隐形位置的信息。最近，OCC3D [25]引入了一个新的任务定义，称为3D占用预测。该任务和SSC之间的主要区别在于，3D占用预测</font>**<font style="color:rgb(0, 0, 0);">仅关注可见区域</font>**<font style="color:rgb(0, 0, 0);">，并针对动态场景进行量身定制。</font>

第三段：<font style="color:#DF2A3F;">（现有的方法是什么，有什么缺陷）</font>现有的3D占用预测方法大多基于密集的BEV方法，例如BEVFormer [15]、BEVDet [7]。这些方法通过将BEV编码器与占用头集成来生成输出，从而增强BEV感知能力以获得更好的结果。然而，它们存在一些共同的缺点。<font style="color:#DF2A3F;">(1)均匀采样：基于BEV的方法无法区分同一场景中的不同区域，将其视为等同对待。这导致了粗略的采样，并阻碍了动态或多分辨率采样的能力。(2)推理灵活性有限：在推理过程中，这些方法只能一次性处理整个场景。它们缺乏根据不同的下游任务或特定的实际需求推断场景不同部分的能力。</font>

第四段：这些限制突显了需要更加灵活的3D占用预测方法，这些方法能够处理复杂场景，同时适应不同的推理需求。本文提出了一种新颖的基于点的3D占用预测表示方法。与现有体积方法将场景划分为均匀网格不同，我们提出了兴趣点（PoIs），将场景视为一系列点的集合，在训练和推理阶段灵活采样场景。图1比较了体积表示和基于点的表示。与体积表示相比，我们的基于点的表示具有以下优势：(1)它可以接受任意尺度和位置的输入来进行占用预测，包括手动设计和自适应设计的输入，提供灵活性；(2)它可以特别关注某些区域，而不是平等地对待所有区域，增强模型的感知能力。

第五段：参考意义不大

### <font style="color:rgb(0, 0, 0);">相关工作</font>
（1）三维占用预测

（2）三维场景语义完成：

基于部分可见区域预测场景的全面语义信息。 二者区别：（1）SSC主要旨在根据可见区域推断不可见区域的占用情况，而三维占用预测则侧重于可见区域；(2) SSC方法通常针对静态场景，而三维占用预测方法则常设计用于处理动态场景。

大多数现有的基于体积的SSC和3D占用预测方法都具有密集性，<font style="color:#DF2A3F;">涵盖了整个场景的输入和输出</font>。以BEVFormer基线为例：它将场景分割成均匀的BEV网格，无法区分不同区域的网格。这种均匀性限制了像BEVFormer这样的基于体积的方法在训练时采样感兴趣区域的能力。此外，如果我们希望在推理阶段专注于特定区域，基于体积的方法只能先推断整个场景，然后再进行后处理，这不可避免地导致成本增加且不必要。更重要的是，随着场景大小和体素分辨率的增加，计算需求呈指数级增长。相比之下，我们的基于点的模型通过关注PoI引入了急需的灵活性。我们的方法促进了特定领域的直接推理，消除了对后处理的需求，并避免了额外的计算负担。

一种与点相关的SSC方法是PointOcc [34]，这是一种基于点云的SSC预测方法，使用三个互补的视图平面进行高效的点云特征建模，并使用高效的2D主干进行处理以减少计算负载，而我们的方法侧重于训练和推理的灵活性。

（3）基于摄像头的三维目标检测：



# GEOcc
### 摘要
<font style="color:rgb(0, 0, 0);">3D占用感知通过将环绕图像转换为密集的3D网格中的集成几何和语义表示，在最近以视觉为中心的自动驾驶系统中起着关键作用。然而，当前的模型仍然遇到两个主要挑战：在2d-3d视图转换阶段准确建模深度，并克服了由于稀疏的激光雷达监督而导致的普遍性问题。为了解决这些问题，本文介绍了GEOCC，这是一个针对Visionally环绕视图的几何增强的占用网络。我们的方法是三个方面：1）基于显式升力的深度预测和基于隐式投影的变压器的整合，以增强视图变换的密度和鲁棒性。 2）利用基于掩码的编码器架构来进行细粒的语义预测； 3）在相关阶段采用上下文感知的自我训练损失函数以补充激光雷达的监督，涉及从3D占用特征中重新构建2D深度图，并利用图像重建损失，以获得更密集的深度监督，除了稀疏的LIDAR地面图。与当前模型相比，我们的方法在OCC3D-Nuscenes数据集上实现了最不需要的图像分辨率和最失重的图像主链的最新性能，这标志着我们提出的贡献，提高了3.3％。全面的实验还证明了我们方法比基线和替代方法的优势一致。</font>

